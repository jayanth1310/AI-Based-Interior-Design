{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayanth1310/AI-Based-Interior-Design/blob/main/AI_Baesd_Interior_Design(latest).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGoI_2o4bkFZ"
      },
      "source": [
        "# AI Based Interior Design Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqqRYYmNVwum",
        "outputId": "d058d46a-8f07-4f63-cda8-c7e47c134c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install streamlit pyngrok diffusers transformers accelerate opencv-python pillow -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBmyihoxV4dQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyngrok import ngrok\n",
        "import streamlit as st\n",
        "import torch\n",
        "from PIL import Image\n",
        "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
        "from transformers import DPTFeatureExtractor, DPTForDepthEstimation\n",
        "import numpy as np\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZpE11tNdnG-",
        "outputId": "02d0df9f-c492-4224-ef1d-281893a93c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from PIL import Image\n",
        "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
        "from transformers import DPTImageProcessor, DPTForDepthEstimation\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "# Fix for CUDA/cuDNN warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow logs\n",
        "torch.backends.cudnn.benchmark = True  # Optimize CUDA\n",
        "\n",
        "# Fix for Streamlit event loop error\n",
        "import asyncio\n",
        "if not hasattr(asyncio, '_nest_patched'):\n",
        "    asyncio._nest_patched = True  # Patch for Colab's async environment\n",
        "\n",
        "def preprocess_image(image):\n",
        "    \"\"\"Preprocess image to match required dimensions.\"\"\"\n",
        "    original_width, original_height = image.size\n",
        "    adjusted_width = adjust_dimensions(original_width)\n",
        "    adjusted_height = adjust_dimensions(original_height)\n",
        "    return image.resize((adjusted_width, adjusted_height))\n",
        "\n",
        "def adjust_dimensions(size):\n",
        "    return size - (size % 8) if (size % 8) != 0 else size\n",
        "\n",
        "def process_and_generate(image, prompt):\n",
        "    \"\"\"Process the image and generate the new design.\"\"\"\n",
        "    resized_image = preprocess_image(image)\n",
        "    depth_map = generate_depth_map(resized_image)\n",
        "    return generate_design(depth_map, prompt, resized_image.size)\n",
        "\n",
        "#streamlit app creation\n",
        "def streamlit_generation():\n",
        "  st.title(\"AI Interior Designer ðŸ›‹ï¸\")\n",
        "  uploaded_image = st.file_uploader(\"Upload a room photo\", type=[\"jpg\", \"png\"])\n",
        "  prompt = st.text_input(\"Design prompt:\")\n",
        "  with st.spinner(\"Generating your design...\"):\n",
        "        output_image = pipe(\n",
        "            prompt=prompt,\n",
        "            image=depth_map,\n",
        "            height=adjusted_height,\n",
        "            width=adjusted_width,\n",
        "            num_inference_steps=30,\n",
        "            guidance_scale=7.5,\n",
        "            controlnet_conditioning_scale=0.8\n",
        "        ).images[0]\n",
        "\n",
        "# Load models\n",
        "@st.cache_resource\n",
        "def load_models():\n",
        "    depth_estimator = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n",
        "    image_processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-large\")\n",
        "\n",
        "    controlnet = ControlNetModel.from_pretrained(\n",
        "        \"lllyasviel/sd-controlnet-depth\",\n",
        "        torch_dtype=torch.float16\n",
        "    )\n",
        "    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\",\n",
        "        controlnet=controlnet,\n",
        "        torch_dtype=torch.float16\n",
        "    ).to(\"cuda\")\n",
        "    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "    return pipe, depth_estimator, image_processor\n",
        "\n",
        "def generate_depth_map(image):\n",
        "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        depth_output = depth_estimator(**inputs).predicted_depth\n",
        "    depth = depth_output.numpy()\n",
        "    depth = (depth - depth.min()) / (depth.max() - depth.min()) * 255.0\n",
        "    depth = depth.astype(np.uint8)\n",
        "    return Image.fromarray(depth.squeeze())\n",
        "\n",
        "def is_valid_prompt(prompt):\n",
        "    \"\"\"Checks if the prompt is related to room design changes.\"\"\"\n",
        "    allowed_keywords = [\n",
        "        \"sofa\", \"table\", \"chair\", \"bed\", \"wall\", \"carpet\", \"curtains\", \"room\", \"furniture\", \"interior\", \"decor\",\n",
        "        \"lamp\", \"bookshelf\", \"painting\", \"ceiling\", \"floor\", \"lighting\", \"couch\", \"vase\", \"mirror\", \"window\",\n",
        "        \"blinds\", \"cushion\", \"coffee table\", \"side table\", \"wardrobe\", \"drapes\", \"wall art\", \"shelves\", \"desk\",\n",
        "        \"television\", \"cabinet\", \"ottoman\", \"chandelier\", \"tiles\", \"color scheme\", \"plants\", \"accent wall\",\n",
        "        \"bedside table\", \"rug\", \"frame\", \"pillows\", \"headboard\", \"nightstand\", \"workspace\", \"storage\",\n",
        "        \"fireplace\", \"doors\", \"bedding\", \"paneling\", \"molding\", \"baseboards\", \"wallpaper\", \"built-in shelves\"\n",
        "    ]\n",
        "    return any(keyword in prompt.lower() for keyword in allowed_keywords)\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"AI Interior Designer ðŸ›‹ï¸\")\n",
        "uploaded_image = st.file_uploader(\"Upload a room photo\", type=[\"jpg\", \"png\"])\n",
        "prompt = st.text_input(\"Design prompt:\")\n",
        "\n",
        "if uploaded_image and prompt:\n",
        "    if not is_valid_prompt(prompt):\n",
        "        st.error(\"Invalid prompt! Please enter a design prompt related to room changes (e.g., sofa, wall, furniture, decor).\")\n",
        "        st.stop()\n",
        "\n",
        "    pipe, depth_estimator, image_processor = load_models()\n",
        "\n",
        "    # Process image and get dimensions\n",
        "    input_image = Image.open(uploaded_image).convert(\"RGB\")\n",
        "    original_width, original_height = input_image.size\n",
        "\n",
        "    # Adjust dimensions to multiples of 8 (required by Stable Diffusion)\n",
        "    def adjust_dimensions(size):\n",
        "        return size - (size % 8) if (size % 8) != 0 else size\n",
        "\n",
        "    adjusted_width = adjust_dimensions(original_width)\n",
        "    adjusted_height = adjust_dimensions(original_height)\n",
        "\n",
        "    # Resize input image to adjusted dimensions (optional but recommended)\n",
        "    input_image = input_image.resize((adjusted_width, adjusted_height))\n",
        "\n",
        "    # Generate depth map\n",
        "    depth_map = generate_depth_map(input_image)\n",
        "\n",
        "    # Generate output image with explicit dimensions\n",
        "    with st.spinner(\"Generating your design...\"):\n",
        "        output_image = pipe(\n",
        "            prompt=prompt,\n",
        "            image=depth_map,\n",
        "            height=adjusted_height,\n",
        "            width=adjusted_width,\n",
        "            num_inference_steps=40,\n",
        "            guidance_scale=7.5,\n",
        "            controlnet_conditioning_scale=0.8\n",
        "        ).images[0]\n",
        "\n",
        "    # Resize output back to original dimensions (optional)\n",
        "    output_image = output_image.resize((original_width, original_height))\n",
        "\n",
        "    # Display results\n",
        "    st.subheader(\"Original Design\")\n",
        "    st.image(input_image, use_container_width=True)\n",
        "\n",
        "    st.subheader(\"New Design\")\n",
        "    st.image(output_image, use_container_width=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLhZwe2jWFjV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04dc6d7c-3ab6-47c4-ea17-a605d5fb5c45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit URL: NgrokTunnel: \"https://03ac-34-126-147-220.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "2025-03-24 17:21:07.111 \n",
            "Warning: the config option 'server.enableCORS=false' is not compatible with 'server.enableXsrfProtection=true'.\n",
            "As a result, 'server.enableCORS' is being overridden to 'true'.\n",
            "\n",
            "More information:\n",
            "In order to protect against CSRF attacks, we send a cookie with each request.\n",
            "To do so, we must specify allowable origins, which places a restriction on\n",
            "cross-origin resource sharing.\n",
            "\n",
            "If cross origin resource sharing is required, please disable server.enableXsrfProtection.\n",
            "            \n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  URL: \u001b[0m\u001b[1mhttp://0.0.0.0:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-03-24 17:21:24.571639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742836884.595497    5620 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742836884.603233    5620 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-24 17:21:30.460 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 345, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "Some weights of DPTForDepthEstimation were not initialized from the model checkpoint at Intel/dpt-large and are newly initialized: ['neck.fusion_stage.layers.0.residual_layer1.convolution1.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution1.weight', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.bias', 'neck.fusion_stage.layers.0.residual_layer1.convolution2.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading pipeline components...: 100% 7/7 [00:28<00:00,  4.11s/it]\n",
            "100% 40/40 [00:23<00:00,  1.71it/s]\n",
            "2025-03-24 17:23:32.875 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 345, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "ngrok.set_auth_token(\"2ui3YOhepTXwGzNHw1MMXsxYgYd_23SXGg8gwPKyR6TVWh9Ji\")\n",
        "public_url = ngrok.connect(addr=\"8501\", proto=\"http\", bind_tls=True)\n",
        "print(\"Streamlit URL:\", public_url)\n",
        "\n",
        "# Run Streamlit in the background\n",
        "!streamlit run app.py --server.port 8501 --browser.serverAddress 0.0.0.0 --server.enableCORS False"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMttssu/rVg7ZGKLZ2q6WzZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}